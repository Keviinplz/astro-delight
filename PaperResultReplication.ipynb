{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En el presente notebook, buscaremos reproducir los resultados obtenidos en el paper de [DELIGHT](https://arxiv.org/pdf/2208.04310).\n",
    "\n",
    "Para esto, trabajaremos adaptando el modelo propuesto a PyTorch, y mediremos su rendimiento bajo las métricas propuestas por el paper.\n",
    "\n",
    "# Métricas\n",
    "\n",
    "Evaluaremos el desempeño de la red mediante 6 métricas sobre el conjunto de test\n",
    "\n",
    "- $$ RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (Y_i - \\hat{Y_i})^2} $$\n",
    "- $$ Mean Deviaton = \\frac{1}{N} \\sum_{i=1}^{N} \\lVert Y_i - \\hat{Y_i} \\rVert $$\n",
    "- $$ Median Deviaton = mediana(\\lVert Y_i - \\hat{Y_i} \\rVert) $$\n",
    "- $$ Mode Deviaton = moda(\\lVert Y_i - \\hat{Y_i} \\rVert) $$\n",
    "\n",
    "Donde:\n",
    "- $N$: Es el tamaño total de los datos del conjunto de test.\n",
    "- $Y_i$: Es un vector 2d que representa la posición real de la galaxia host. $Y_i = (x_i, y_i)$\n",
    "- $\\hat{Y_i }$: Es un vector 2d que representa la posición predicha de la galaxia host. $\\hat{Y_i} = (\\hat{x_i}, \\hat{y_i})$\n",
    "\n",
    "# Baseline\n",
    "\n",
    "Los valores obtenidos para cada métrica en el paper son:\n",
    "- RMSE: 1.836 ± 0.05100\n",
    "- Mean Deviation: 0.783 ± 0.00900\n",
    "- Median Deviation: 0.468 ± 0.00800\n",
    "- Mode Deviation: 0.427 ± 0.05100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from typing import Callable\n",
    "from scipy import stats  # type: ignore\n",
    "from sklearn.utils import resample  # type: ignore\n",
    "\n",
    "StatisticFunction = Callable[[npt.NDArray[np.float32]], float]\n",
    "\n",
    "\n",
    "def bootstrap_statistic(\n",
    "    data: npt.NDArray[np.float32],\n",
    "    statistic: StatisticFunction,\n",
    "    n_iterations: int = 1000,\n",
    ") -> float:\n",
    "    stats = np.zeros(n_iterations)\n",
    "    for i in range(n_iterations):\n",
    "        sample: npt.NDArray[np.float32] = resample(data)  # type: ignore\n",
    "        stats[i] = statistic(sample)\n",
    "    return np.std(stats).item()\n",
    "\n",
    "\n",
    "def rmse(\n",
    "    y_true: npt.NDArray[np.float32], y_pred: npt.NDArray[np.float32]\n",
    ") -> tuple[float, float]:\n",
    "    has_shape_2 = len(y_true.shape) == len(y_pred.shape) == 2\n",
    "    are_points = y_true.shape[1] == y_pred.shape[1] == 2\n",
    "    assert (\n",
    "        has_shape_2 and are_points\n",
    "    ), f\"Expected vectors of dim (N, 2): y_true={y_true.shape} y_pred={y_pred.shape}\"\n",
    "\n",
    "    sum_distance_squared: npt.NDArray[np.float32] = np.sum(\n",
    "        (y_true - y_pred) ** 2, axis=1\n",
    "    )\n",
    "    value = np.sqrt(np.mean(sum_distance_squared))  # type: ignore\n",
    "    assert isinstance(value, float), f\"Expected float result: {value}\"\n",
    "    return value, bootstrap_statistic(\n",
    "        sum_distance_squared, lambda x: np.sqrt(np.mean(x))\n",
    "    )\n",
    "\n",
    "\n",
    "def mean_deviation(\n",
    "    y_true: npt.NDArray[np.float32], y_pred: npt.NDArray[np.float32]\n",
    ") -> tuple[float, float]:\n",
    "    has_shape_2 = len(y_true.shape) == len(y_pred.shape) == 2\n",
    "    are_points = y_true.shape[1] == y_pred.shape[1] == 2\n",
    "    assert (\n",
    "        has_shape_2 and are_points\n",
    "    ), f\"Expected vectors of dim (N, 2): y_true={y_true.shape} y_pred={y_pred.shape}\"\n",
    "\n",
    "    deviation: npt.NDArray[np.float32] = np.linalg.norm(y_true - y_pred, axis=1)  # type: ignore\n",
    "    return np.mean(deviation).item(), bootstrap_statistic(deviation, np.mean)\n",
    "\n",
    "\n",
    "def median_deviation(\n",
    "    y_true: npt.NDArray[np.float32], y_pred: npt.NDArray[np.float32]\n",
    ") -> tuple[float, float]:\n",
    "    has_shape_2 = len(y_true.shape) == len(y_pred.shape) == 2\n",
    "    are_points = y_true.shape[1] == y_pred.shape[1] == 2\n",
    "    assert (\n",
    "        has_shape_2 and are_points\n",
    "    ), f\"Expected vectors of dim (N, 2): y_true={y_true.shape} y_pred={y_pred.shape}\"\n",
    "\n",
    "    deviation: npt.NDArray[np.float32] = np.linalg.norm(y_true - y_pred, axis=1)  # type: ignore\n",
    "    return np.median(deviation).item(), bootstrap_statistic(deviation, np.median)\n",
    "\n",
    "\n",
    "def mode_deviation(\n",
    "    y_true: npt.NDArray[np.float32], y_pred: npt.NDArray[np.float32]\n",
    ") -> tuple[float, float]:\n",
    "    has_shape_2 = len(y_true.shape) == len(y_pred.shape) == 2\n",
    "    are_points = y_true.shape[1] == y_pred.shape[1] == 2\n",
    "    assert (\n",
    "        has_shape_2 and are_points\n",
    "    ), f\"Expected vectors of dim (N, 2): y_true={y_true.shape} y_pred={y_pred.shape}\"\n",
    "\n",
    "    deviation: npt.NDArray[np.float32] = np.linalg.norm(y_true - y_pred, axis=1)  # type: ignore\n",
    "    mode = stats.mode(deviation, axis=None).mode  # type: ignore\n",
    "    return mode, bootstrap_statistic(\n",
    "        deviation,\n",
    "        lambda x: stats.mode(x).mode,  # type: ignore\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "from typing import TypedDict\n",
    "from functools import reduce\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class DelightCnnParameters(TypedDict):\n",
    "    nconv1: int\n",
    "    nconv2: int\n",
    "    nconv3: int\n",
    "    ndense: int\n",
    "    levels: int\n",
    "    dropout: float\n",
    "    rot: bool\n",
    "    flip: bool\n",
    "\n",
    "\n",
    "class RotationAndFlipLayer(torch.nn.Module):\n",
    "    def __init__(self, rot: bool = True, flip: bool = True):\n",
    "        super().__init__()  # type: ignore\n",
    "        self.rot = rot\n",
    "        self.flip = flip\n",
    "        self.n_transforms = (int(flip) + 1) * (3 * int(rot) + 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        stacked = reduce(lambda x, y: x * y, x.shape[:-3], 1)\n",
    "\n",
    "        if self.rot is False and self.flip is False:\n",
    "            x = x.reshape(stacked, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "            return x\n",
    "\n",
    "        w_dim = len(x.shape) - 2\n",
    "        h_dim = len(x.shape) - 1\n",
    "        transforms: tuple[torch.Tensor, ...]\n",
    "\n",
    "        if self.rot is False:\n",
    "            flipped = x.flip(dims=(h_dim,))\n",
    "            transforms = (x, flipped)\n",
    "\n",
    "        elif self.flip is False:\n",
    "            rot90 = x.rot90(k=1, dims=(w_dim, h_dim))\n",
    "            rot180 = x.rot90(k=2, dims=(w_dim, h_dim))\n",
    "            rot270 = x.rot90(k=3, dims=(w_dim, h_dim))\n",
    "            transforms = (x, rot90, rot180, rot270)\n",
    "\n",
    "        else:\n",
    "            rot90 = x.rot90(k=1, dims=(w_dim, h_dim))\n",
    "            rot180 = x.rot90(k=2, dims=(w_dim, h_dim))\n",
    "            rot270 = x.rot90(k=3, dims=(w_dim, h_dim))\n",
    "            flipped = x.flip(dims=(h_dim,))\n",
    "            flipped_rot90 = flipped.rot90(k=1, dims=(w_dim, h_dim))\n",
    "            flipped_rot180 = flipped.rot90(k=2, dims=(w_dim, h_dim))\n",
    "            flipped_rot270 = flipped.rot90(k=3, dims=(w_dim, h_dim))\n",
    "            transforms = (\n",
    "                x,\n",
    "                rot90,\n",
    "                rot180,\n",
    "                rot270,\n",
    "                flipped,\n",
    "                flipped_rot90,\n",
    "                flipped_rot180,\n",
    "                flipped_rot270,\n",
    "            )\n",
    "\n",
    "        x = torch.cat(transforms, dim=1)\n",
    "        return x.reshape(\n",
    "            stacked * self.n_transforms, x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "        )\n",
    "\n",
    "\n",
    "class DelightCnn(torch.nn.Module):\n",
    "    def __init__(self, options: DelightCnnParameters):\n",
    "        super().__init__()  # type: ignore\n",
    "        bottleneck: OrderedDict[str, torch.nn.Module] = OrderedDict(\n",
    "            [\n",
    "                (\"conv1\", torch.nn.Conv2d(1, options[\"nconv1\"], 3)),\n",
    "                (\"relu1\", torch.nn.ReLU()),\n",
    "                (\"mp1\", torch.nn.MaxPool2d(2)),\n",
    "                (\"conv2\", torch.nn.Conv2d(options[\"nconv1\"], options[\"nconv2\"], 3)),\n",
    "                (\"relu2\", torch.nn.ReLU()),\n",
    "                (\"mp2\", torch.nn.MaxPool2d(2)),\n",
    "                (\"conv3\", torch.nn.Conv2d(options[\"nconv2\"], options[\"nconv3\"], 3)),\n",
    "                (\"relu3\", torch.nn.ReLU()),\n",
    "                (\"flatten\", torch.nn.Flatten()),\n",
    "            ]\n",
    "        )\n",
    "        linear_in = self._compute_dense_features(\n",
    "            levels=options[\"levels\"], bottleneck=bottleneck\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(\n",
    "            in_features=linear_in, out_features=options[\"ndense\"]\n",
    "        )\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.dropout = torch.nn.Dropout(p=options[\"dropout\"])\n",
    "        self.fc2 = torch.nn.Linear(in_features=options[\"ndense\"], out_features=2)\n",
    "        self.rot_and_flip = RotationAndFlipLayer(\n",
    "            rot=options[\"rot\"], flip=options[\"flip\"]\n",
    "        )\n",
    "        self.bottleneck = torch.nn.Sequential(bottleneck)\n",
    "\n",
    "    def _compute_dense_features(\n",
    "        self,\n",
    "        *,\n",
    "        bottleneck: OrderedDict[str, torch.nn.Module],\n",
    "        levels: int,\n",
    "    ) -> int:\n",
    "        w = 30\n",
    "        h = 30\n",
    "        conv_out = 0\n",
    "        for layer in bottleneck.values():\n",
    "            k: int\n",
    "            if isinstance(layer, torch.nn.Conv2d):\n",
    "                k = layer.kernel_size[0]\n",
    "                w = w - k + 1\n",
    "                h = h - k + 1\n",
    "                conv_out = layer.out_channels\n",
    "            if isinstance(layer, torch.nn.MaxPool2d):\n",
    "                k = layer.kernel_size  # type: ignore\n",
    "                w = math.floor((w - k) / 2 + 1)\n",
    "                h = math.floor((h - k) / 2 + 1)\n",
    "\n",
    "        return w * h * conv_out * levels\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch = x.shape[0]  # TODO: Remove batch dependency\n",
    "\n",
    "        # Apply flips and rotations over level (L) dimension\n",
    "        x = self.rot_and_flip(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Undo transformations\n",
    "        x = x.reshape(batch, self.rot_and_flip.n_transforms, -1)\n",
    "\n",
    "        # Linear\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.reshape(batch, self.rot_and_flip.n_transforms * 2)\n",
    "\n",
    "    def derotate(self, y_pred: torch.Tensor) -> npt.NDArray[np.float32]:\n",
    "        y_pred_numpy: npt.NDArray[np.float32] = y_pred.cpu().numpy()\n",
    "        return (\n",
    "            np.dstack(\n",
    "                [\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 0],\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 1, ::-1]\n",
    "                    * [1, -1],\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 2, :]\n",
    "                    * [-1, -1],\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 3, ::-1]\n",
    "                    * [-1, 1],\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 4, :]\n",
    "                    * [1, -1],\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 5, ::-1],\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 6, :]\n",
    "                    * [-1, 1],\n",
    "                    y_pred_numpy.reshape((y_pred_numpy.shape[0], 8, 2))[:, 7, ::-1]\n",
    "                    * [-1, -1],\n",
    "                ]\n",
    "            )\n",
    "            .reshape((y_pred_numpy.shape[0], 2, 8))\n",
    "            .swapaxes(1, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 01:38:48.649839: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-14 01:38:48.649901: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-14 01:38:48.649933: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-14 01:38:48.661629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 01:38:50.112906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Any, cast\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp  # type: ignore\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DelightDatasetType(Enum):\n",
    "    TRAIN = \"TRAIN\"\n",
    "    TEST = \"TEST\"\n",
    "    VALIDATION = \"VALIDATION\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DelightDatasetOptions:\n",
    "    source: str\n",
    "    n_levels: int\n",
    "    fold: int\n",
    "    mask: bool\n",
    "    object: bool\n",
    "    rot: bool\n",
    "    flip: bool\n",
    "\n",
    "    def get_filenames(self, datatype: DelightDatasetType) -> tuple[str, str]:\n",
    "        if datatype == DelightDatasetType.TRAIN:\n",
    "            x = \"X_train_nlevels%i_fold%i_mask%s_objects%s.npy\" % (\n",
    "                self.n_levels,\n",
    "                self.fold,\n",
    "                self.mask,\n",
    "                self.object,\n",
    "            )\n",
    "            y = \"y_train_nlevels%i_fold%i_mask%s_objects%s.npy\" % (\n",
    "                self.n_levels,\n",
    "                self.fold,\n",
    "                self.mask,\n",
    "                self.object,\n",
    "            )\n",
    "        elif datatype == DelightDatasetType.TEST:\n",
    "            x = \"X_test_nlevels%i_mask%s_objects%s.npy\" % (\n",
    "                self.n_levels,\n",
    "                self.mask,\n",
    "                self.object,\n",
    "            )\n",
    "            y = \"y_test_nlevels%i_mask%s_objects%s.npy\" % (\n",
    "                self.n_levels,\n",
    "                self.mask,\n",
    "                self.object,\n",
    "            )\n",
    "        else:\n",
    "            x = \"X_val_nlevels%i_fold%i_mask%s_objects%s.npy\" % (\n",
    "                self.n_levels,\n",
    "                self.fold,\n",
    "                self.mask,\n",
    "                self.object,\n",
    "            )\n",
    "            y = \"y_val_nlevels%i_fold%i_mask%s_objects%s.npy\" % (\n",
    "                self.n_levels,\n",
    "                self.fold,\n",
    "                self.mask,\n",
    "                self.object,\n",
    "            )\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class DelightDataset(Dataset[tuple[torch.Tensor, torch.Tensor]]):\n",
    "    def __init__(self, options: DelightDatasetOptions, datatype: DelightDatasetType):\n",
    "        X_path, y_path = options.get_filenames(datatype)\n",
    "        self.X = torch.Tensor(np.load(os.path.join(options.source, X_path))).permute(\n",
    "            0, 3, 1, 2\n",
    "        )\n",
    "\n",
    "        y = np.load(os.path.join(options.source, y_path))\n",
    "        self.y = (\n",
    "            self.transform(\n",
    "                y,\n",
    "                options.rot,\n",
    "                options.flip,\n",
    "            )\n",
    "            if datatype != DelightDatasetType.TEST\n",
    "            else torch.from_numpy(y)  # type: ignore\n",
    "        )\n",
    "        self.y_raw = np.load(os.path.join(options.source, y_path))\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(\n",
    "        y: np.ndarray[Any, np.dtype[np.float32]], rot: bool, flip: bool\n",
    "    ) -> torch.Tensor:\n",
    "        transformed: tuple[np.ndarray[Any, np.dtype[np.float32]], ...]\n",
    "\n",
    "        if rot is False and flip is False:\n",
    "            return torch.Tensor(y)\n",
    "\n",
    "        yflip = cast(np.ndarray[Any, np.dtype[np.float32]], [1, -1] * y)\n",
    "        if rot is False:\n",
    "            transformed = (y, yflip)\n",
    "\n",
    "        y90 = cast(np.ndarray[Any, np.dtype[np.float32]], [-1, 1] * y[:, ::-1])\n",
    "        y180 = cast(np.ndarray[Any, np.dtype[np.float32]], [-1, 1] * y90[:, ::-1])\n",
    "        y270 = cast(np.ndarray[Any, np.dtype[np.float32]], [-1, 1] * y180[:, ::-1])\n",
    "        yflip90 = cast(np.ndarray[Any, np.dtype[np.float32]], [-1, 1] * yflip[:, ::-1])\n",
    "        yflip180 = cast(\n",
    "            np.ndarray[Any, np.dtype[np.float32]], [-1, 1] * yflip90[:, ::-1]\n",
    "        )\n",
    "        yflip270 = cast(\n",
    "            np.ndarray[Any, np.dtype[np.float32]], [-1, 1] * yflip180[:, ::-1]\n",
    "        )\n",
    "\n",
    "        if flip is False:\n",
    "            transformed = (y, y90, y180, y270)\n",
    "        else:\n",
    "            transformed = (y, y90, y180, y270, yflip, yflip90, yflip180, yflip270)\n",
    "\n",
    "        return torch.Tensor(np.concatenate(transformed, axis=1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        if len(x.shape) == 3:  # has no channel information\n",
    "            levels, width, height = x.shape\n",
    "            x = x.reshape(levels, 1, width, height)  # asume 1 channel information\n",
    "        return x, y\n",
    "\n",
    "    def to_tf_dataset(self) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        X = cast(np.ndarray[Any, np.dtype[np.float32]], self.X.numpy())\n",
    "        y = cast(np.ndarray[Any, np.dtype[np.float32]], self.y.numpy())\n",
    "\n",
    "        return tnp.copy(X.transpose((0, 2, 3, 1))), tnp.copy(y)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from typing import TypedDict\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from ray import train\n",
    "from ray.train import Checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "class HyperParameters(TypedDict):\n",
    "    lr: float\n",
    "    batch_size: int | float\n",
    "    nconv1: int | float\n",
    "    nconv2: int | float\n",
    "    nconv3: int | float\n",
    "    ndense: int | float\n",
    "    dropout: float\n",
    "    epochs: int\n",
    "\n",
    "\n",
    "class EvaluationResult(TypedDict):\n",
    "    rmse: tuple[float, float]\n",
    "    mean_deviation: tuple[float, float]\n",
    "    median_deviation: tuple[float, float]\n",
    "    mode_deviation: tuple[float, float]\n",
    "\n",
    "\n",
    "def _get_value_from_parameter(parameter: int | float, base: int = 2) -> int:\n",
    "    return int(base**parameter) if isinstance(parameter, float) else parameter\n",
    "\n",
    "\n",
    "def get_delight_cnn_parameters(\n",
    "    params: HyperParameters, options: DelightDatasetOptions\n",
    ") -> DelightCnnParameters:\n",
    "    return {\n",
    "        \"nconv1\": _get_value_from_parameter(params[\"nconv1\"]),\n",
    "        \"nconv2\": _get_value_from_parameter(params[\"nconv2\"]),\n",
    "        \"nconv3\": _get_value_from_parameter(params[\"nconv3\"]),\n",
    "        \"ndense\": _get_value_from_parameter(params[\"ndense\"]),\n",
    "        \"levels\": options.n_levels,\n",
    "        \"dropout\": params[\"dropout\"],\n",
    "        \"rot\": options.rot,\n",
    "        \"flip\": options.flip,\n",
    "    }\n",
    "\n",
    "\n",
    "def _train_one_epoch(\n",
    "    *,\n",
    "    device: str,\n",
    "    train_dl: DataLoader[tuple[torch.Tensor, torch.Tensor]],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    model: DelightCnn,\n",
    "    criterion: torch.nn.MSELoss,\n",
    "    writer: SummaryWriter,\n",
    "    is_ray: bool,\n",
    "    epoch: int\n",
    "):\n",
    "    running_loss = 0.\n",
    "    inputs: torch.Tensor\n",
    "    positions: torch.Tensor\n",
    "    outputs: torch.Tensor\n",
    "    loss: torch.Tensor\n",
    "\n",
    "    model.train()\n",
    "    for i, (inputs, positions) in enumerate(train_dl):\n",
    "        inputs, positions = inputs.to(device), positions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, positions)\n",
    "        loss.backward()  # type: ignore\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_value = loss.item() \n",
    "\n",
    "        if is_ray is False:\n",
    "            t = epoch * len(train_dl) + i # type: ignore\n",
    "            writer.add_scalar(\"[MSE Loss]: Train\", loss_value, t)  # type: ignore\n",
    "\n",
    "        running_loss += loss_value * inputs.size(0)\n",
    "\n",
    "    return running_loss / len(train_dl.dataset)  # type: ignore\n",
    "\n",
    "\n",
    "def _validate_train(\n",
    "    *,\n",
    "    device: str,\n",
    "    val_dl: DataLoader[tuple[torch.Tensor, torch.Tensor]],\n",
    "    model: DelightCnn,\n",
    "    criterion: torch.nn.MSELoss,\n",
    "):\n",
    "    running_loss = 0.\n",
    "    data: tuple[torch.Tensor, torch.Tensor]\n",
    "    outputs: torch.Tensor\n",
    "    loss: torch.Tensor\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(val_dl):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    return running_loss / len(val_dl.dataset) # type: ignore\n",
    "\n",
    "\n",
    "def _train(\n",
    "    *,\n",
    "    start_epoch: int,\n",
    "    num_epochs: int,\n",
    "    device: str,\n",
    "    train_dl: DataLoader[tuple[torch.Tensor, torch.Tensor]],\n",
    "    val_dl: DataLoader[tuple[torch.Tensor, torch.Tensor]],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    model: DelightCnn,\n",
    "    criterion: torch.nn.MSELoss,\n",
    "    is_ray: bool = False,\n",
    "):\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter(comment=datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%SZ\"))\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_loss = _train_one_epoch(\n",
    "            device=device,\n",
    "            train_dl=train_dl,\n",
    "            optimizer=optimizer,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            is_ray=is_ray,\n",
    "            writer=writer,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "        val_loss = _validate_train(\n",
    "            device=device, val_dl=val_dl, model=model, criterion=criterion\n",
    "        )\n",
    "\n",
    "        print(f\"[EPOCH {epoch+1}] train loss = {train_loss} | val_loss = {val_loss}\")\n",
    "        metrics = {\"val_loss\": val_loss, \"train_loss\": train_loss}\n",
    "        \n",
    "        if is_ray is False:\n",
    "            writer.add_scalars(\"[MSE Loss]: Train / Validation\", metrics, epoch)  # type: ignore\n",
    "            continue\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            torch.save(  # type: ignore\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"net_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                os.path.join(tempdir, \"checkpoint.pt\"),\n",
    "            )\n",
    "            train.report(metrics=metrics, checkpoint=Checkpoint.from_directory(tempdir))  # type: ignore\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "def train_delight_cnn_model(\n",
    "    params: HyperParameters, options: DelightDatasetOptions\n",
    ") -> DelightCnn:\n",
    "    device = \"cpu\" if torch.cuda.is_available() is False else \"cuda\"\n",
    "    batch_size = _get_value_from_parameter(params[\"batch_size\"])\n",
    "    net_options = get_delight_cnn_parameters(params, options)\n",
    "    net = DelightCnn(net_options)\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=params[\"lr\"], weight_decay=1e-4)\n",
    "    checkpoint = cast(Checkpoint | None, train.get_checkpoint())  # type: ignore\n",
    "    start_epoch = 0\n",
    "\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_dict = torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))  # type: ignore\n",
    "            start_epoch = int(checkpoint_dict[\"epoch\"]) + 1\n",
    "            net.load_state_dict(checkpoint_dict[\"net_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_dict[\"optimizer_state_dict\"])\n",
    "\n",
    "    train_dataset = DelightDataset(options=options, datatype=DelightDatasetType.TRAIN)\n",
    "    val_dataset = DelightDataset(\n",
    "        options=options, datatype=DelightDatasetType.VALIDATION\n",
    "    )\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\n",
    "        \"Starting: epochs=%s,batch_size=%s,lr=%s,nconv1=%s,nconv2=%s,nconv3=%s,ndense=%s,dropout=%s\"\n",
    "        % (\n",
    "            params[\"epochs\"],\n",
    "            batch_size,\n",
    "            params[\"lr\"],\n",
    "            net_options[\"nconv1\"],\n",
    "            net_options[\"nconv2\"],\n",
    "            net_options[\"nconv3\"],\n",
    "            net_options[\"ndense\"],\n",
    "            net_options[\"dropout\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    _train(\n",
    "        start_epoch=start_epoch,\n",
    "        num_epochs=params[\"epochs\"],\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        optimizer=optimizer,\n",
    "        model=net,\n",
    "        criterion=criterion,\n",
    "        is_ray=checkpoint is not None,\n",
    "    )\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def evaluate_delight_cnn_model(\n",
    "    model: DelightCnn, options: DelightDatasetOptions\n",
    ") -> EvaluationResult:\n",
    "    device = \"cpu\" if torch.cuda.is_available() is False else \"cuda\"\n",
    "    test_dataset = DelightDataset(options=options, datatype=DelightDatasetType.TEST)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    predictions: list[tuple[float, float]] = []\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    inputs: torch.Tensor\n",
    "    outputs: torch.Tensor\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(test_dl):\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            derotated = model.derotate(outputs)\n",
    "            y_hat: npt.NDArray[np.float32] = np.mean(derotated, axis=1)\n",
    "            predictions.extend(y_hat.tolist())\n",
    "\n",
    "    y_true: npt.NDArray[np.float32] = test_dataset.y.cpu().numpy()\n",
    "    y_pred: npt.NDArray[np.float32] = np.array(predictions)\n",
    "\n",
    "    return {\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"mean_deviation\": mean_deviation(y_true, y_pred),\n",
    "        \"median_deviation\": median_deviation(y_true, y_pred),\n",
    "        \"mode_deviation\": mode_deviation(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 158132), started 0:05:43 ago. (Use '!kill 158132' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cb44a78dcde77441\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cb44a78dcde77441\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: epochs=50,batch_size=40,lr=0.0014,nconv1=52,nconv2=57,nconv3=41,ndense=685,dropout=0.06\n",
      "[EPOCH 1] train loss = 252.0790331670234 | val_loss = 219.78536707793148\n",
      "[EPOCH 2] train loss = 128.32898105003758 | val_loss = 169.39433623988066\n",
      "[EPOCH 3] train loss = 107.8085772997386 | val_loss = 152.15092055346943\n",
      "[EPOCH 4] train loss = 95.75713814949381 | val_loss = 107.3790698150711\n",
      "[EPOCH 5] train loss = 82.04846020827127 | val_loss = 96.13334832362192\n",
      "[EPOCH 6] train loss = 72.78262585863851 | val_loss = 83.45232794743394\n",
      "[EPOCH 7] train loss = 65.6467670130891 | val_loss = 76.52644894462541\n",
      "[EPOCH 8] train loss = 58.91023311485914 | val_loss = 73.56522845189637\n",
      "[EPOCH 9] train loss = 53.11343988659852 | val_loss = 70.41715534124445\n",
      "[EPOCH 10] train loss = 48.8168454837948 | val_loss = 67.26101846242328\n",
      "[EPOCH 11] train loss = 44.83924540351419 | val_loss = 64.83426743919506\n",
      "[EPOCH 12] train loss = 41.96766791375958 | val_loss = 61.76436128862494\n",
      "[EPOCH 13] train loss = 38.97007810343435 | val_loss = 62.03178113227482\n",
      "[EPOCH 14] train loss = 37.07028465082346 | val_loss = 61.26675042939325\n",
      "[EPOCH 15] train loss = 35.22646110421478 | val_loss = 61.40572921600469\n",
      "[EPOCH 16] train loss = 34.14932055078653 | val_loss = 61.12967321854845\n",
      "[EPOCH 17] train loss = 32.54464179559774 | val_loss = 59.737925109418605\n",
      "[EPOCH 18] train loss = 31.50468660927514 | val_loss = 59.717408889338536\n",
      "[EPOCH 19] train loss = 30.32061124331997 | val_loss = 63.14363340493741\n",
      "[EPOCH 20] train loss = 29.478326836208204 | val_loss = 60.766934608440415\n",
      "[EPOCH 21] train loss = 28.612560018532477 | val_loss = 63.106454298160756\n",
      "[EPOCH 22] train loss = 27.75764548089714 | val_loss = 62.510292253327506\n",
      "[EPOCH 23] train loss = 27.128387424363748 | val_loss = 63.30962541756483\n",
      "[EPOCH 24] train loss = 26.88477881162009 | val_loss = 63.78609129868379\n",
      "[EPOCH 25] train loss = 26.049747122010483 | val_loss = 60.657360799505156\n",
      "[EPOCH 26] train loss = 25.631466788590295 | val_loss = 60.59415819801756\n",
      "[EPOCH 27] train loss = 24.698410717290493 | val_loss = 60.718698377712485\n",
      "[EPOCH 28] train loss = 24.095731151904495 | val_loss = 58.49605832191232\n",
      "[EPOCH 29] train loss = 23.602390636819404 | val_loss = 59.39207200583173\n",
      "[EPOCH 30] train loss = 22.930006707353805 | val_loss = 59.31763514948328\n",
      "[EPOCH 31] train loss = 22.752740190278608 | val_loss = 57.276970540157066\n",
      "[EPOCH 32] train loss = 22.670587186202724 | val_loss = 56.38265758549343\n",
      "[EPOCH 33] train loss = 22.555793712065903 | val_loss = 57.942133215047434\n",
      "[EPOCH 34] train loss = 22.235495505265927 | val_loss = 57.41777657152314\n",
      "[EPOCH 35] train loss = 21.701883484831953 | val_loss = 58.52424775789818\n",
      "[EPOCH 36] train loss = 21.590183364792704 | val_loss = 62.634693566607396\n",
      "[EPOCH 37] train loss = 21.257614282193995 | val_loss = 63.308927975923794\n",
      "[EPOCH 38] train loss = 20.96381676538359 | val_loss = 59.35769395089765\n",
      "[EPOCH 39] train loss = 20.182373424174575 | val_loss = 58.62178716572198\n",
      "[EPOCH 40] train loss = 19.784680759701985 | val_loss = 58.103241322538835\n",
      "[EPOCH 41] train loss = 19.494858415098253 | val_loss = 58.0821584071049\n",
      "[EPOCH 42] train loss = 19.076994930686336 | val_loss = 57.5300814781856\n",
      "[EPOCH 43] train loss = 18.664010045926812 | val_loss = 57.135138298053725\n",
      "[EPOCH 44] train loss = 18.588382613714757 | val_loss = 58.46851067936093\n",
      "[EPOCH 45] train loss = 18.19248883864459 | val_loss = 60.994684130424865\n",
      "[EPOCH 46] train loss = 17.810781685900153 | val_loss = 60.70595863558271\n",
      "[EPOCH 47] train loss = 17.766743366076636 | val_loss = 61.1157664522144\n",
      "[EPOCH 48] train loss = 17.456914515497783 | val_loss = 57.64953593588392\n",
      "[EPOCH 49] train loss = 17.07004094905744 | val_loss = 57.857061652914865\n",
      "[EPOCH 50] train loss = 16.929833660393815 | val_loss = 56.67208434779082\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=runs\n",
    "\n",
    "options = DelightDatasetOptions(\n",
    "    source=os.path.join(os.getcwd(), \"data\"),\n",
    "    n_levels=5,\n",
    "    fold=0,\n",
    "    mask=False,\n",
    "    object=True,\n",
    "    rot=True,\n",
    "    flip=True,\n",
    ")\n",
    "\n",
    "params_paper: HyperParameters = {\n",
    "    \"nconv1\": 52,\n",
    "    \"nconv2\": 57,\n",
    "    \"nconv3\": 41,\n",
    "    \"ndense\": 685,\n",
    "    \"dropout\": 0.06,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 40,\n",
    "    \"lr\": 0.0014,\n",
    "}\n",
    "\n",
    "params: HyperParameters = {\n",
    "    \"nconv1\": 16,\n",
    "    \"nconv2\": 32,\n",
    "    \"nconv3\": 32,\n",
    "    \"ndense\": 128,\n",
    "    \"dropout\": 0,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.0014,\n",
    "}\n",
    "\n",
    "model = train_delight_cnn_model(params_paper, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': (8.29558022797704, 0.6221508076956249),\n",
       " 'mean_deviation': (3.698574798348921, 0.10675042863577326),\n",
       " 'median_deviation': (2.009651905374769, 0.02004303353270777),\n",
       " 'mode_deviation': (2.9544196874954225, 4.743186788967148)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = evaluate_delight_cnn_model(model, options)\n",
    "\n",
    "evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delight-6BEqGedw-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
