{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from utils.stoppers import EarlyStopper\n",
    "from models.delightcnn.model import DelightCnnParameters\n",
    "from models.delightcnn.dataset import DelightDataset, DelightDatasetOptions\n",
    "from models.delightcnn.training import execute_train_model\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s %(levelname)s]: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stderr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSetProcessor:\n",
    "    def __init__(self, source: str, balance: bool = False):\n",
    "        self._source = source\n",
    "        self._balanced_indexes: npt.NDArray[np.int32] | None = None\n",
    "        if balance:\n",
    "            self._balanced_indexes = np.random.shuffle(self._get_balanced_indexes())\n",
    "\n",
    "    def _get_balanced_indexes(self) -> npt.NDArray[np.int32]:\n",
    "        id_train_filepath = os.path.join(self._source, \"id_train.npy\")\n",
    "        id_train: npt.NDArray[np.str_] = np.load(id_train_filepath, allow_pickle=True)\n",
    "        idxAsiago = np.array(\n",
    "            [i for i in range(id_train.shape[0]) if id_train[i][:2] == \"SN\"]\n",
    "        )\n",
    "        idxZTF = np.array(\n",
    "            [i for i in range(id_train.shape[0]) if id_train[i][:3] == \"ZTF\"]\n",
    "        )\n",
    "        nimb = int(idxZTF.shape[0] / idxAsiago.shape[0])\n",
    "\n",
    "        idxbal = np.array([], dtype=int)\n",
    "        for i in range(nimb + 1):\n",
    "            idxbal = np.concatenate([idxbal, idxAsiago])\n",
    "            idxbal = np.concatenate(\n",
    "                [\n",
    "                    idxbal,\n",
    "                    idxZTF[\n",
    "                        i * idxAsiago.shape[0] : min(\n",
    "                            idxZTF.shape[0], (i + 1) * idxAsiago.shape[0]\n",
    "                        )\n",
    "                    ],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return idxbal\n",
    "\n",
    "    @property\n",
    "    def X(self) -> npt.NDArray[np.float32]:\n",
    "        x_train_filepath = os.path.join(self._source, \"X_train.npy\")\n",
    "        X_train: npt.NDArray[np.float32] = np.load(x_train_filepath)\n",
    "\n",
    "        if self._balanced_indexes is not None:\n",
    "            X_train = X_train[self._balanced_indexes]\n",
    "\n",
    "        return X_train.swapaxes(3, 1).swapaxes(2, 3)\n",
    "\n",
    "    @property\n",
    "    def y(self) -> npt.NDArray[np.float32]:\n",
    "        y_train_filepath = os.path.join(self._source, \"y_train.npy\")\n",
    "        y_train: npt.NDArray[np.float32] = np.load(y_train_filepath)\n",
    "\n",
    "        if self._balanced_indexes is not None:\n",
    "            y_train = y_train[self._balanced_indexes]\n",
    "\n",
    "        return y_train\n",
    "\n",
    "\n",
    "class ValidationSetProcessor:\n",
    "    def __init__(self, source: str, pixscale_mask_value: float | None = None):\n",
    "        self._source = source\n",
    "        self._pixscale_mask: npt.NDArray[np.int32] | None = None\n",
    "        if pixscale_mask_value is not None:\n",
    "            self._pixscale_mask = self._get_distance_mask(pixscale_mask_value)\n",
    "\n",
    "    def _get_distance_mask(self, pixscale: float) -> npt.NDArray[np.int32]:\n",
    "        y_validation_filepath = os.path.join(self._source, \"y_validation.npy\")\n",
    "        y_validation: npt.NDArray[np.float32] = np.load(y_validation_filepath)\n",
    "\n",
    "        distance = np.sqrt(np.sum(y_validation**2, axis=1))\n",
    "        return (distance * pixscale) < 60\n",
    "\n",
    "    @property\n",
    "    def X(self) -> npt.NDArray[np.float32]:\n",
    "        x_validation_filepath = os.path.join(self._source, \"X_validation.npy\")\n",
    "        X_validation: npt.NDArray[np.float32] = np.load(x_validation_filepath)\n",
    "\n",
    "        if self._pixscale_mask is not None:\n",
    "            X_validation = X_validation[self._pixscale_mask]\n",
    "\n",
    "        return X_validation.swapaxes(3, 1).swapaxes(2, 3)\n",
    "\n",
    "    @property\n",
    "    def y(self) -> npt.NDArray[np.float32]:\n",
    "        y_validation_filepath = os.path.join(self._source, \"y_validation.npy\")\n",
    "        y_validation: npt.NDArray[np.float32] = np.load(y_validation_filepath)\n",
    "\n",
    "        if self._pixscale_mask is not None:\n",
    "            y_validation = y_validation[self._pixscale_mask]\n",
    "\n",
    "        return y_validation\n",
    "\n",
    "\n",
    "class TestingSetProcessor:\n",
    "    def __init__(self, source: str):\n",
    "        self._source = source\n",
    "\n",
    "    @property\n",
    "    def X(self) -> npt.NDArray[np.float32]:\n",
    "        x_test_filepath = os.path.join(self._source, \"X_test.npy\")\n",
    "        x_test: npt.NDArray[np.float32] = np.load(x_test_filepath)\n",
    "        return x_test.swapaxes(3, 1).swapaxes(2, 1)\n",
    "\n",
    "    @property\n",
    "    def y(self) -> npt.NDArray[np.float32]:\n",
    "        y_test_filepath = os.path.join(self._source, \"y_test.npy\")\n",
    "        return np.load(y_test_filepath)\n",
    "\n",
    "\n",
    "class ProductionTrainingSetProcessor:\n",
    "    def __init__(self, source: str):\n",
    "        self._source = source\n",
    "        self._training_set = TrainingSetProcessor(source)\n",
    "        self._validation_set = ValidationSetProcessor(source)\n",
    "\n",
    "    @property\n",
    "    def X(self) -> npt.NDArray[np.float32]:\n",
    "        return np.concatenate((self._training_set.X, self._validation_set.X))\n",
    "\n",
    "    @property\n",
    "    def y(self) -> npt.NDArray[np.float32]:\n",
    "        return np.concatenate((self._training_set.y, self._validation_set.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset settigns\n",
    "source = os.path.join(os.getcwd(), \"data\")\n",
    "dataset_options = DelightDatasetOptions(channels=1, levels=5, rot=True, flip=True)\n",
    "balance_training_set = True\n",
    "validation_pixscale_mask_value = 0.25\n",
    "\n",
    "# Model settings\n",
    "model_parameters = DelightCnnParameters(\n",
    "    nconv1=16,\n",
    "    nconv2=16,\n",
    "    nconv3=32,\n",
    "    ndense=128,\n",
    "    dropout=0.06,\n",
    "    channels=dataset_options.channels,\n",
    "    levels=dataset_options.levels,\n",
    "    rot=dataset_options.rot,\n",
    "    flip=dataset_options.flip,\n",
    ")\n",
    "\n",
    "# Training settings\n",
    "device: torch.device = torch.device(\"mps\")\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "adam_learning_rate = 0.0014\n",
    "adam_weight_decay = 1e-4\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = partial(\n",
    "    torch.optim.Adam,  # type: ignore\n",
    "    lr=adam_learning_rate,\n",
    "    weight_decay=adam_weight_decay,\n",
    ")\n",
    "stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "writter = SummaryWriter()\n",
    "\n",
    "train_dataset = DelightDataset(\n",
    "    processor=TrainingSetProcessor(source, balance=balance_training_set),\n",
    "    options=dataset_options,\n",
    ")\n",
    "val_dataset = DelightDataset(\n",
    "    processor=ValidationSetProcessor(\n",
    "        source, pixscale_mask_value=validation_pixscale_mask_value\n",
    "    ),\n",
    "    options=dataset_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-28 17:36:31,300 INFO]: [EPOCH 1] train loss = 2415.5647735595703 | val_loss = 309.1363386027018\n",
      "[2024-10-28 17:36:31,300 INFO]: Validation loss has been improved from inf -> 309.1363386027018\n",
      "[2024-10-28 17:36:41,528 INFO]: [EPOCH 2] train loss = 1727.4650440216064 | val_loss = 216.06668968200682\n",
      "[2024-10-28 17:36:41,530 INFO]: Validation loss has been improved from 309.1363386027018 -> 216.06668968200682\n",
      "[2024-10-28 17:36:51,558 INFO]: [EPOCH 3] train loss = 1347.4812803268433 | val_loss = 184.02650428771972\n",
      "[2024-10-28 17:36:51,558 INFO]: Validation loss has been improved from 216.06668968200682 -> 184.02650428771972\n",
      "[2024-10-28 17:37:01,588 INFO]: [EPOCH 4] train loss = 1110.7901306152344 | val_loss = 161.72652524312338\n",
      "[2024-10-28 17:37:01,588 INFO]: Validation loss has been improved from 184.02650428771972 -> 161.72652524312338\n",
      "[2024-10-28 17:37:11,490 INFO]: [EPOCH 5] train loss = 938.5985555648804 | val_loss = 145.32816317240398\n",
      "[2024-10-28 17:37:11,490 INFO]: Validation loss has been improved from 161.72652524312338 -> 145.32816317240398\n",
      "[2024-10-28 17:37:21,357 INFO]: [EPOCH 6] train loss = 797.7822790145874 | val_loss = 117.81436347961426\n",
      "[2024-10-28 17:37:21,358 INFO]: Validation loss has been improved from 145.32816317240398 -> 117.81436347961426\n",
      "[2024-10-28 17:37:30,936 INFO]: [EPOCH 7] train loss = 694.1154737472534 | val_loss = 112.21748807271321\n",
      "[2024-10-28 17:37:30,936 INFO]: Validation loss has been improved from 117.81436347961426 -> 112.21748807271321\n",
      "[2024-10-28 17:37:40,598 INFO]: [EPOCH 8] train loss = 609.8880310058594 | val_loss = 100.4064176305135\n",
      "[2024-10-28 17:37:40,598 INFO]: Validation loss has been improved from 112.21748807271321 -> 100.4064176305135\n",
      "[2024-10-28 17:37:50,477 INFO]: [EPOCH 9] train loss = 538.6866097450256 | val_loss = 92.51735407511393\n",
      "[2024-10-28 17:37:50,478 INFO]: Validation loss has been improved from 100.4064176305135 -> 92.51735407511393\n",
      "[2024-10-28 17:38:00,305 INFO]: [EPOCH 10] train loss = 474.42532992362976 | val_loss = 88.56517659505208\n",
      "[2024-10-28 17:38:00,307 INFO]: Validation loss has been improved from 92.51735407511393 -> 88.56517659505208\n"
     ]
    }
   ],
   "source": [
    "model = execute_train_model(\n",
    "    model_parameters=model_parameters,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,  # type: ignore\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    stopper=stopper,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refactorized-2m7BYJ1-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
